# Sentiment Analysis with Multi-Embedding Fusion and BERT Optimization

This project implements a progressive pipeline for sentiment analysis, starting from traditional models like TF-IDF and advancing to powerful contextual embedding techniques like BERT. By evaluating each method on a custom-labeled dataset of Google Play Store reviews, this project highlights the strengths, limitations, and improvements across various NLP techniques.

## ğŸ“Œ Project Highlights

- Comparative analysis of **TF-IDF**, **Word2Vec**, **GloVe**, **FastText**, and **BERT** models.
- Final pipeline integrates **BERT embeddings** with **Support Vector Machine (SVM)** for high accuracy.
- Custom preprocessing pipeline that preserves negation and handles noisy, real-world text data.
- Misclassification analysis for every model to understand model limitations and error patterns.
- Achieved **97.5% accuracy** using BERT + SVM on real-world review sentiment classification.

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ cleaned_dataset.csv             # Preprocessed dataset used for training
â”œâ”€â”€ modified_dataset.csv            # Original labeled dataset
â”œâ”€â”€ tfidf_misclassified_data.csv    # Misclassifications (TF-IDF)
â”œâ”€â”€ word2vec_misclassified_data.csv
â”œâ”€â”€ glove_misclassified_data.csv
â”œâ”€â”€ fasttext_misclassified_data.csv
â”œâ”€â”€ bert_misclassified_data.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tfidf_lr_model.pkl
â”‚   â”œâ”€â”€ word2vec_lr_model.pkl
â”‚   â”œâ”€â”€ glove_lr_model.pkl
â”‚   â”œâ”€â”€ fasttext_lr_model.pkl
â”‚   â””â”€â”€ bert_svm_model.pkl
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ training_pipeline.py
â”‚   â””â”€â”€ evaluation.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸ§  Models Compared

| Model                  | Accuracy | F1-Score | Strengths                                     | Limitations                            |
|------------------------|----------|----------|-----------------------------------------------|-----------------------------------------|
| TF-IDF + Logistic Reg. | 83%      | 0.83     | Fast, interpretable, strong baseline          | Ignores context, fails on negation      |
| Word2Vec + LR          | 52%      | 0.40     | Captures semantic similarity                  | No context, fails on short texts        |
| GloVe + LR             | 81%      | 0.81     | Uses global co-occurrence statistics          | Still context-agnostic                  |
| FastText + LR          | 85%      | 0.85     | Handles rare/misspelled words(subword n-grams)| Cannot model sentence context           |
| BERT + SVM             | 97.5%    | 0.97     | Deep contextual understanding, robust to noise| Requires GPU, high computational cost   |

## ğŸ› ï¸ Preprocessing Pipeline

- Lowercasing
- URL & HTML tag removal
- Punctuation stripping
- Contraction expansion
- Stopword removal (negations preserved)
- Tokenization
- Lemmatization
- Duplicate & null record removal

## ğŸ§ª Evaluation Strategy

- **Accuracy**, **Precision**, **Recall**, **F1-score** calculated per model
- Deep misclassification analysis per technique
- Focused on how models handle negation, sarcasm, OOV words, and contextual ambiguity

## ğŸ“Š Model Architecture

![image](https://github.com/user-attachments/assets/49a8c42a-e3ea-4f85-baed-d888de5ff7cf)

## ğŸ“ˆ Final Results

- The **BERT + SVM** hybrid model demonstrated **state-of-the-art performance** by using [CLS] token embeddings as input to an SVM classifier.
- Common issues like sarcasm, idioms, and negations were better handled with contextual embeddings.

## ğŸ” Future Work

- Fine-tuning BERT on domain-specific data (e.g., BERTweet for social reviews)
- Implementing ensemble models combining traditional + deep learning
- Adding real-time inference and deployment (Flask/Streamlit UI)
- Augmenting data with paraphrased and sarcastic samples
 

## ğŸ“¦ Requirements

```bash
transformers==4.x
scikit-learn==1.x
nltk==3.x
pandas==1.x
numpy==1.x
gensim==4.x
fasttext
contractions
```

## ğŸ“œ License

This project is part of the B.Tech curriculum at Shiv Nadar University and is intended for academic and educational purposes.



