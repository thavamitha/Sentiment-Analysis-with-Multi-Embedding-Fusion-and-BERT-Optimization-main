{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jsana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jsana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jsana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"modified_dataset.csv\")  \n",
    "\n",
    "# 1. Convert to Lowercase\n",
    "df['review'] = df['review'].astype(str).str.lower()\n",
    "\n",
    "# 2. Remove URLs\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "df['review'] = df['review'].apply(remove_urls)\n",
    "\n",
    "# 3. Remove HTML Tags\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "\n",
    "# 4. Remove Punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "df['review'] = df['review'].apply(remove_punctuation)\n",
    "\n",
    "# 5. Handle Contractions (Optional Improvement)\n",
    "from contractions import fix\n",
    "df['review'] = df['review'].apply(fix)  # Example: \"don't\" → \"do not\"\n",
    "\n",
    "# 6. Remove Stopwords (But Keep Negation Words)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "negation_words = {\"not\", \"no\", \"never\", \"none\", \"nothing\", \"nowhere\", \"neither\", \"hardly\", \"barely\"}\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words or word in negation_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df['review'] = df['review'].apply(remove_stopwords)\n",
    "\n",
    "# 7. Tokenization\n",
    "df['tokenized_review'] = df['review'].apply(word_tokenize)\n",
    "\n",
    "# 8. Lemmatization (Preferred over Stemming)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(tokens):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "df['lemmatized_review'] = df['tokenized_review'].apply(lemmatize_words)\n",
    "\n",
    "# 9. Drop duplicates & missing values (Final cleanup)\n",
    "df.drop_duplicates(subset=['lemmatized_review'], inplace=True)\n",
    "df.dropna(subset=['lemmatized_review'], inplace=True)\n",
    "\n",
    "# Save cleaned dataset\n",
    "df.to_csv(\"cleaned_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_review</th>\n",
       "      <th>lemmatized_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wow loved place</td>\n",
       "      <td>positive</td>\n",
       "      <td>[wow, loved, place]</td>\n",
       "      <td>wow loved place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crust not good</td>\n",
       "      <td>negative</td>\n",
       "      <td>[crust, not, good]</td>\n",
       "      <td>crust not good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not tasty texture nasty</td>\n",
       "      <td>negative</td>\n",
       "      <td>[not, tasty, texture, nasty]</td>\n",
       "      <td>not tasty texture nasty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stopped late may bank holiday rick steve recom...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[stopped, late, may, bank, holiday, rick, stev...</td>\n",
       "      <td>stopped late may bank holiday rick steve recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selection menu great prices</td>\n",
       "      <td>positive</td>\n",
       "      <td>[selection, menu, great, prices]</td>\n",
       "      <td>selection menu great price</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0                                    wow loved place  positive   \n",
       "1                                     crust not good  negative   \n",
       "2                            not tasty texture nasty  negative   \n",
       "3  stopped late may bank holiday rick steve recom...  positive   \n",
       "4                        selection menu great prices  positive   \n",
       "\n",
       "                                    tokenized_review  \\\n",
       "0                                [wow, loved, place]   \n",
       "1                                 [crust, not, good]   \n",
       "2                       [not, tasty, texture, nasty]   \n",
       "3  [stopped, late, may, bank, holiday, rick, stev...   \n",
       "4                   [selection, menu, great, prices]   \n",
       "\n",
       "                                   lemmatized_review  \n",
       "0                                    wow loved place  \n",
       "1                                     crust not good  \n",
       "2                            not tasty texture nasty  \n",
       "3  stopped late may bank holiday rick steve recom...  \n",
       "4                         selection menu great price  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       100\n",
      "           1       0.86      0.78      0.82        98\n",
      "\n",
      "    accuracy                           0.83       198\n",
      "   macro avg       0.83      0.83      0.83       198\n",
      "weighted avg       0.83      0.83      0.83       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\") \n",
    "\n",
    "# Convert labels to numerical format (assuming binary classification: positive/negative)\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})  # Modify if more classes exist\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))  # Includes unigrams & bigrams\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Logistic Regression Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df = pd.DataFrame({'review': X_test, 'actual_sentiment': y_test, 'predicted_sentiment': y_pred})\n",
    "predictions_df.to_csv(\"tf_idf_predictions.csv\", index=False)\n",
    "\n",
    "# Analyze Misclassified Data\n",
    "misclassified_df = predictions_df[predictions_df['actual_sentiment'] != predictions_df['predicted_sentiment']].copy()\n",
    "\n",
    "# Identify possible misclassification reasons\n",
    "def analyze_misclassification(review):\n",
    "    if re.search(r'\\b(not|never|no|none|hardly|barely|scarcely)\\b', review):\n",
    "        return \"Possible Negation Issue\"\n",
    "    elif len(review.split()) < 5:\n",
    "        return \"Too Short for Meaningful Classification\"\n",
    "    elif re.search(r'(:-\\)|:-\\()', review):  # Detect emoticons\n",
    "        return \"Possible Sarcasm\"\n",
    "    else:\n",
    "        return \"Unclear\"\n",
    "\n",
    "misclassified_df['misclassification_reason'] = misclassified_df['review'].apply(analyze_misclassification)\n",
    "\n",
    "# Save misclassified reviews\n",
    "misclassified_df.to_csv(\"tf_idf_misclassified_data.csv\", index=False)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Model Performance:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jsana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5202020202020202\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.07      0.13        99\n",
      "    positive       0.51      0.97      0.67        99\n",
      "\n",
      "    accuracy                           0.52       198\n",
      "   macro avg       0.61      0.52      0.40       198\n",
      "weighted avg       0.61      0.52      0.40       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download NLTK tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")  # Ensure preprocessed dataset is used\n",
    "\n",
    "# 1. Tokenization\n",
    "df['tokenized_review'] = df['review'].apply(word_tokenize)\n",
    "\n",
    "# 2. Train Word2Vec Model\n",
    "word2vec_model = Word2Vec(sentences=df['tokenized_review'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "word_vectors = word2vec_model.wv\n",
    "\n",
    "# 3. One-Hot Encode Words (Fixed)\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")  # ✅ Fix applied\n",
    "unique_words = list(word_vectors.index_to_key)  # Unique words from Word2Vec vocabulary\n",
    "word_array = np.array(unique_words).reshape(-1, 1)\n",
    "encoder.fit(word_array)\n",
    "\n",
    "# 4. Convert Reviews into Word2Vec Averages\n",
    "def review_to_vector(tokens):\n",
    "    vectors = [word_vectors[word] for word in tokens if word in word_vectors]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)  # Handle empty cases\n",
    "\n",
    "df['review_vector'] = df['tokenized_review'].apply(review_to_vector)\n",
    "\n",
    "# 5. Prepare Data\n",
    "X = np.vstack(df['review_vector'])\n",
    "y = df['sentiment']\n",
    "\n",
    "# 6. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 7. Train Logistic Regression Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 8. Make Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 9. Save Predictions\n",
    "df_test = df.iloc[y_test.index].copy()\n",
    "df_test['predicted_sentiment'] = y_pred\n",
    "df_test.to_csv(\"word2vec_predictions.csv\", index=False)\n",
    "\n",
    "# 10. Identify Misclassified Samples\n",
    "misclassified = df_test[df_test['sentiment'] != df_test['predicted_sentiment']].copy()\n",
    "\n",
    "# Function to determine misclassification reasons\n",
    "def get_misclassification_reason(row):\n",
    "    true_sentiment = row['sentiment']\n",
    "    predicted_sentiment = row['predicted_sentiment']\n",
    "    review_text = row['review']\n",
    "\n",
    "    if true_sentiment == 'positive' and predicted_sentiment == 'negative':\n",
    "        if \"not\" in review_text or \"but\" in review_text:\n",
    "            return \"Misinterpreted contrast or negation.\"\n",
    "        elif any(word in review_text for word in ['sarcasm', 'joke', 'irony']):\n",
    "            return \"Possible sarcasm detection failure.\"\n",
    "        else:\n",
    "            return \"Failed to recognize positive sentiment words.\"\n",
    "\n",
    "    elif true_sentiment == 'negative' and predicted_sentiment == 'positive':\n",
    "        if any(word in review_text for word in ['disappointed', 'worst', 'awful']):\n",
    "            return \"Negative sentiment possibly diluted by surrounding words.\"\n",
    "        elif len(review_text.split()) < 5:\n",
    "            return \"Short negative review misclassified.\"\n",
    "        else:\n",
    "            return \"Failed to capture intensity of negative sentiment.\"\n",
    "\n",
    "    return \"Unclear misclassification reason.\"\n",
    "\n",
    "# Apply the function\n",
    "misclassified['reason'] = misclassified.apply(get_misclassification_reason, axis=1)\n",
    "\n",
    "# 11. Save Misclassified Data\n",
    "misclassified.to_csv(\"misclassified_word2vec.csv\", index=False)\n",
    "\n",
    "# 12. Print Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Path to the downloaded GloVe file\n",
    "glove_path = \"glove.6B.100d.txt\"\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = {}\n",
    "\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]  # First value is the word\n",
    "        vector = np.array(values[1:], dtype=\"float32\")  # Remaining values are vector components\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "print(\"GloVe embeddings loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jsana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8080808080808081\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.77      0.80        99\n",
      "    positive       0.79      0.85      0.82        99\n",
      "\n",
      "    accuracy                           0.81       198\n",
      "   macro avg       0.81      0.81      0.81       198\n",
      "weighted avg       0.81      0.81      0.81       198\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsana\\AppData\\Local\\Temp\\ipykernel_22964\\2522070391.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  misclassified[\"reason\"] = np.where(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download tokenizer\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")  # Make sure this file is preprocessed\n",
    "df[\"tokenized_review\"] = df[\"review\"].apply(word_tokenize)\n",
    "\n",
    "# Function to convert a review into a GloVe vector (average of word embeddings)\n",
    "def review_to_glove_vector(tokens):\n",
    "    vectors = [glove_embeddings[word] for word in tokens if word in glove_embeddings]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)  # Handle empty cases\n",
    "\n",
    "# Convert each review to a vector\n",
    "df[\"review_vector\"] = df[\"tokenized_review\"].apply(review_to_glove_vector)\n",
    "\n",
    "# Prepare Data for Model\n",
    "X = np.vstack(df[\"review_vector\"])\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train Logistic Regression Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save Predictions\n",
    "df_test = df.iloc[y_test.index].copy()\n",
    "df_test[\"predicted_sentiment\"] = y_pred\n",
    "df_test.to_csv(\"glove_predictions.csv\", index=False)\n",
    "\n",
    "# Identify Misclassified Samples\n",
    "misclassified = df_test[df_test[\"sentiment\"] != df_test[\"predicted_sentiment\"]]\n",
    "misclassified[\"reason\"] = np.where(\n",
    "    misclassified[\"sentiment\"] == \"positive\",\n",
    "    \"Misclassified positive review: Model failed to detect positive sentiment.\",\n",
    "    \"Misclassified negative review: Model failed to detect negative sentiment.\"\n",
    ")\n",
    "\n",
    "# Save Misclassified Data\n",
    "misclassified.to_csv(\"misclassified_glove.csv\", index=False)\n",
    "\n",
    "# Print Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText embeddings ready for use!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Define URL and filenames\n",
    "fasttext_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n",
    "fasttext_file = \"cc.en.300.vec\"\n",
    "\n",
    "# Download FastText embeddings if not already present\n",
    "if not os.path.exists(fasttext_file):\n",
    "    print(\"Downloading FastText embeddings... (This may take a while)\")\n",
    "    urllib.request.urlretrieve(fasttext_url, fasttext_file + \".gz\")\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "    # Extract the file\n",
    "    import gzip\n",
    "    import shutil\n",
    "\n",
    "    with gzip.open(fasttext_file + \".gz\", \"rb\") as f_in, open(fasttext_file, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "print(\"FastText embeddings ready for use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jsana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FastText embeddings... This may take a while.\n",
      "FastText embeddings loaded successfully!\n",
      "Accuracy: 0.8484848484848485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.84      0.85        99\n",
      "    positive       0.84      0.86      0.85        99\n",
      "\n",
      "    accuracy                           0.85       198\n",
      "   macro avg       0.85      0.85      0.85       198\n",
      "weighted avg       0.85      0.85      0.85       198\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsana\\AppData\\Local\\Temp\\ipykernel_22964\\3850233931.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  misclassified[\"reason\"] = np.where(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download tokenizer\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")  # Make sure this file is preprocessed\n",
    "df[\"tokenized_review\"] = df[\"review\"].apply(word_tokenize)\n",
    "\n",
    "# Load FastText Embeddings\n",
    "fasttext_embeddings = {}\n",
    "\n",
    "print(\"Loading FastText embeddings... This may take a while.\")\n",
    "with open(\"cc.en.300.vec\", \"r\", encoding=\"utf-8\") as f:\n",
    "    next(f)  # Skip first line (metadata)\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]  # First value is the word\n",
    "        vector = np.array(values[1:], dtype=\"float32\")  # Remaining values are vector components\n",
    "        fasttext_embeddings[word] = vector\n",
    "print(\"FastText embeddings loaded successfully!\")\n",
    "\n",
    "# Function to convert a review into a FastText vector (average of word embeddings)\n",
    "def review_to_fasttext_vector(tokens):\n",
    "    vectors = [fasttext_embeddings[word] for word in tokens if word in fasttext_embeddings]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)  # Handle empty cases\n",
    "\n",
    "# Convert each review to a vector\n",
    "df[\"review_vector\"] = df[\"tokenized_review\"].apply(review_to_fasttext_vector)\n",
    "\n",
    "# Prepare Data for Model\n",
    "X = np.vstack(df[\"review_vector\"])\n",
    "y = df[\"sentiment\"]\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train Logistic Regression Model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save Predictions\n",
    "df_test = df.iloc[y_test.index].copy()\n",
    "df_test[\"predicted_sentiment\"] = y_pred\n",
    "df_test.to_csv(\"fasttext_predictions.csv\", index=False)\n",
    "\n",
    "# Identify Misclassified Samples\n",
    "misclassified = df_test[df_test[\"sentiment\"] != df_test[\"predicted_sentiment\"]]\n",
    "misclassified[\"reason\"] = np.where(\n",
    "    misclassified[\"sentiment\"] == \"positive\",\n",
    "    \"Misclassified positive review: Model failed to detect positivity, possibly due to sarcasm or complex phrasing.\",\n",
    "    \"Misclassified negative review: Model failed to capture negativity, likely due to subtle sentiment or domain-specific words.\"\n",
    ")\n",
    "\n",
    "# Save Misclassified Data\n",
    "misclassified.to_csv(\"misclassified_fasttext.csv\", index=False)\n",
    "\n",
    "# Print Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jsana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4980\n",
      "Epoch 2, Loss: 0.1467\n",
      "Epoch 3, Loss: 0.0528\n",
      "SVM Accuracy: 0.9750\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"modified_dataset.csv\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, reviews, sentiments, tokenizer, max_len=128):\n",
    "        self.reviews = reviews\n",
    "        self.sentiments = sentiments\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.reviews[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(self.sentiments[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Prepare dataset\n",
    "df[\"sentiment\"] = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"review\"], df[\"sentiment\"], test_size=0.2, random_state=42)\n",
    "train_dataset = SentimentDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
    "test_dataset = SentimentDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define Model\n",
    "class BERT_NN(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BERT_NN, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(256, 2)  # Output layer for classification\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.fc(cls_embedding)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x, cls_embedding\n",
    "\n",
    "# Load Pretrained BERT\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = BERT_NN(bert_model).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Extract BERT Embeddings for SVM\n",
    "model.eval()\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device)\n",
    "        _, cls_embeddings = model(input_ids, attention_mask)\n",
    "        all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "X_train_svm = np.concatenate(all_embeddings, axis=0)\n",
    "y_train_svm = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Train SVM\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "# Evaluate SVM\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device)\n",
    "        _, cls_embeddings = model(input_ids, attention_mask)\n",
    "        all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "X_test_svm = np.concatenate(all_embeddings, axis=0)\n",
    "y_test_svm = np.concatenate(all_labels, axis=0)\n",
    "y_pred_svm = svm.predict(X_test_svm)\n",
    "accuracy = accuracy_score(y_test_svm, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted and misclassified reviews\n",
    "df_test = pd.DataFrame({\"review\": X_test.tolist(), \"actual\": y_test_svm, \"predicted\": y_pred_svm})\n",
    "df_test.to_csv(\"bert_svm_predictions.csv\", index=False)\n",
    "\n",
    "misclassified = df_test[df_test[\"actual\"] != df_test[\"predicted\"]]\n",
    "misclassified.to_csv(\"bert_svm_misclassified_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
